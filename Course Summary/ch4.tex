\subsection{Latent semantic indexing} % (fold)
\label{sub:latent_semantic_indexing}
  \subsubsection{Introduction} % (fold)
  \label{ssub:introduction}
    \textbf{Vector Space Retrieval} model problems: It's based on \textbf{index terms}, so there are problems due to \emph{synonyms} ($\Rightarrow$ miss relevant documents, poor recall) and \emph{homonyms} ($\Rightarrow$ include unrelated documents, poor precision).

    A \textbf{solution} is to use higher-level \textbf{concepts} instead, with each concept represented by a combination of terms in the \emph{concept space}. There are much fewer concepts than terms, and the dimensionality of the term space is given by the \emph{vocabulary} size. Both query terms and documents are mapped to the concept space.

    \textbf{Similarity in concept space} is computed by scalar product of \emph{normalized concept vectors}. \emph{Concepts} are represented by sets of terms. \emph{Documents} are represented by \emph{concept vectors} that count the number of \emph{concept terms} in the document. The \emph{concept vectors} are then normalized, and we compute the \emph{cosine similarities} among the resulting concept vectors.

    \textbf{Concepts identification and computation} can be done manually by users annotating documents using terms of an \emph{ontology}, or can be done \emph{automatically}:
    \begin{itemize}
      \item $M_{ij}$ is a term-document matrix with $m$ rows (terms) and $n$ columns (documents). $M^t$ has a document in each row, and these rows should be normalized to 1.
      \item Each element of $M_{ij}$ is assigned a weight $w_{ij}$ associated with $t_{i}$ and $d_j$. The weight can be based on TF-IDF.
    \end{itemize}

    The \textbf{ranking} can be computed as $M^t \cdot q$, with $q$ being the \emph{query}.
  % subsubsection introduction (end)

  \subsubsection{Identifying top concepts} % (fold)
  \label{ssub:identifying_top_concepts}
    Key idea: Extract the essential features of $M^t$ and approximate it by the most important ones.

    \textbf{Singular Value Decomposition (SVD)} (always exists and is unique):
    \begin{itemize}
      \item $M = K\cdot S\cdot D^t$
      \item $S$ a $r\times r$ diagonal matrix of the singular values in decreasing order, $r=\min{(m,n)}$ (rank of M)
      \item $K$ is the matrix of eigenvectors of $M\cdot M^t$
      \item $D$ is the matrix of eigenvectors of $M^t\cdot M$
      \item Algorithms have complexity $O(n^3)$ if $m\leq n$
      \item \emph{Interpretation}:
      \begin{itemize}
        \item Rewrite $M$ as \emph{sum of outer vector products}: $\sum_{i=1}^{r}{s_i k_i \times d_i^t}$ (sum of components weighted by the singular values)
        \item $s_i$ ordered in decreasing size.
        \item \emph{Least square approximation} is obtained by taking the largest $s_i$
        \item $d_i$ is the $i$-th row of $D$
        \item \emph{Geometrical interpretation of singular values} -- lengths of sexi-axes of hyperellipsoid $E=\{Mx\quad |\quad  ||x||_2=1\}$. We can interpret the axes of $E$ as the dimensions of the concept space.
      \end{itemize}
    \end{itemize}

    \begin{figure}[htp]
      \centering
        \includegraphics[width=.7\textwidth]{images/SVD1.png}
        %\missingfigure[figwidth=\textwidth]{Some Figure}
        \caption{SVD illustration 1}
        \label{fig:svd1}
    \end{figure}

    \begin{figure}[htp]
      \centering
        \includegraphics[width=.7\textwidth]{images/SVD2.png}
        %\missingfigure[figwidth=\textwidth]{Some Figure}
        \caption{SVD illustration 2}
        \label{fig:svd2}
    \end{figure}

    \textbf{Latent Semantic Indexing (LSI)}: $M_s = K_s \cdot S_s\cdot D_s^t$ with $s<r$ the dimensionality of the concept space (large enough to allow fitting the data, small enough to filter details). Rows in $K_s$ correspond to \emph{term vectors}, columns in $D_s^t$ correspond to document vectors. By using \emph{cosine similarity} between columns of $D_s^t$ the \emph{similarity of documents} can be computed.

    \begin{figure}[htp]
      \centering
        \includegraphics[width=.7\textwidth]{images/LSI.png}
        %\missingfigure[figwidth=\textwidth]{Some Figure}
        \caption{LSI illustration}
        \label{fig:LSI}
    \end{figure}

    \textbf{Answering queries}: After SVD, documents can be compared by computing \emph{cosine similarity} in the \emph{concept space} (comparing columns $(D_s^t)_i$ and $(D_s^t)_j$ of matrix $D_s^t$). Queries are treated like another document (is added as an additional column to $M$, and then the same transformation is applied as for mapping $M$ to $D$).

    To map $M$ to $D$:
    \begin{enumerate}
      \item $M=K\cdot S\cdot D^t \Rightarrow D = M^t\cdot K\cdot S^{-1}$
      \item Apply same transformation to query $q$: $q^* = q^t\cdot K_s\cdot S_s^{-1}$
      \item Compare using standard cosine measure: $$sim(q^*, d_i)-\frac{q^*\cdot(D_s^t)_i}{|q^*|\cdot|(D_s^t)_i|}$$
    \end{enumerate}

    \begin{figure}[htp]
      \centering
        \includegraphics[width=.7\textwidth]{images/LSIq.png}
        %\missingfigure[figwidth=\textwidth]{Some Figure}
        \caption{LSI querying illustration}
        \label{fig:LSIq}
    \end{figure}

    \textbf{Discussion of LSI}: Allows reducing complexity of concept representation and facilitates interfacing with user. Computationally expensive and poor statistical explanation (i.e. poor explanatory power, because the LS approximation assumes normally distributed term frequencies, but term frequencies are power law distributed). \textbf{Alternatives to LSI}:
    \begin{itemize}
      \item Probabilistic Latent Semantic Analysis -- based on Bayesian Networks
      \item Latent Dirichlet Allocation (LDA) -- based on Dirichlet Allocation, state-of-the-art for concept extraction. Better explained mathematical foundation and better experimental results.
    \end{itemize}

    \textbf{Latent Dirichlet Allocation (LDA)}: Assumes a document collection is (randomly) generated from a known set of topics (probabilistic generative model, similar to probabilistic language model used in information retrieval).

    \emph{LDA Document generation using a probabilistic process} -- (1) For each topic, choose a mixture of topics. (2) For each word position, sample a topic from the topic mixture. (3) For every word position, sample a word from the chosen topic.

    \emph{LDA Topic Identification} -- Given a document collection, reconstruct the (probabilistic) topic model that has generated the document collection. Distributions follow a Dirichlet distribution.

    \textbf{Use of Topic Models}: \emph{Unsupervised Learning} (understand main topics of a topic collection, organize a document collection); \emph{Information retrieval} (use topic vectors instead of term vectors to represent documents and queries); \emph{Supervised learning} (document classification using topics as features).
  % subsubsection identifying_top_concepts (end)
% subsection latent_semantic_indexing (end)

\subsection{Word embeddings} % (fold)
\label{sub:word_embeddings}
  The neighborhood of a word expresses a lot about its meaning (Firth, 1957). We can consider all the neighborhoods that we can find in all documents of a large document collection.

  For each word $w$ we can identify its \emph{context} $C(w)$ (``neighboring words'' choosing a certain number of preceding and succeeding words, e.g. 5).

  \textbf{Similarity based representation} -- Two words are \emph{similar} if they have similar contexts. Advantages compared to co-occurrence methods like LSI:
  \begin{itemize}
    \item The context captures \emph{synctactic} and \emph{semantic} similarity.
    \item It distinguishes if a word occurs as the \emph{center of a context} and as a \emph{member of a context}.
  \end{itemize}

  \textbf{Approach}:
  \begin{itemize}
    \item Words and context-words are mapped into the same low-dimensional space (e.g. $d = 200$). Their representations in the space are similar if word and context-word often occur together.
    \item The \emph{vector distance} (product) measures how likely the word and its context are likely to occur together. Similar words and contexts should be close (thanks to dimensionality reduction).
  \end{itemize}

  \textbf{Model overview}:
  \begin{itemize}
    \item The two $W$ matrices map words and context words
    \item Mapping a word (word vector $w$): $V_w = W^{(w)}\cdot w$
    \item Mapping a context (context vector $c$): $V_c = W^{(c)}\cdot c$
    \item Model parameters ($\theta$) are the coefficients of $W^{(w)}$ and $W^{(c)}$
    \item $W^{(w)}$ is a $m\times d$ matrix with $m$ rows and $d$ columns, with each row representing a word of the vocabulary in the concept space of dimension $d$.
    \item In $W^{(c)}$, each column represents a dimension in the concept space -- how relevant word $c$ is for each concept, how often context-word $c$ occurs with all words, word $c$ represented in concept space.
  \end{itemize}

  The model is learned from the data. Intuition: convert similarity value in vector space into a probability. Consider a pair $(w,c)$ and get probability that it comes from the data $$p(D=1|w,c;\theta)=1/(1+e^{-v_c\cdot v_w})=\sigma(v_c\cdot v_w)$$

  Problem: finding parameters $\theta$
  \begin{itemize}
    \item Formulate an optimization problem
    \item Assume we have positive examples $D$ for $(w,c)$ and negative examples $\hat{D}$ (not in document collection).
    \item Find $\theta$ that maximizes overall probability $$\theta=\text{argmax}_\theta \prod_{(w,c)\in D}{P(D=1|w,c,\theta)}\prod_{(w,c)\in \hat{D}}{P(D=0|w,c,\theta)}$$
    \item The previous can be expressed as $$\text{argmax}_\theta\sum_{(w,c)\in D}{\log{\sigma(v_c\cdot v_w)}} + \sum_{(w,c)\in D'}{\log{\sigma(-v_c\cdot v_w)}}$$
    \item Loss function to be minimized: $J(\theta)=1/m\cdot\sum_{t=1}^{m}{J_t(\theta)}$, with $J_t(\theta)=-\log{(\sigma(v_c\cdot v_w))}-\sum_{k=1}^{K}{\log{(\sigma(v_{c_k}\cdot v_w)}}$
    \item $v_{c_k}$ are negative examples of context words taken from a set $P_n(w)=V\\C(w)$. Negative examples can be obtained by choosing any word from vocabulary $V$ that is not contained in the context. For $p_w$ the probability of word $w$ in collection, choose the word with probability $p_w^{3/4}$. Less frequent words are sampled more often. In practice the probability is approximated by sampling a few non-context words.
    \item Given the loss function, optimal $\theta$ is obtained using SGD iterating for every $(w,c)\in D$, using $$\theta^{(t+1)}=\theta^t-\alpha\Delta_\theta J_t(\theta)$$
  \end{itemize}

  \textbf{Result}: Matrices $W^{(w)}$ and $W^{(c)}$ that capture information on word similarity. Words appearing in similar contexts generate similar contexts and viceversa $\Rightarrow$ mapped to similar representations in lower dimensional space. Use $W=W^{(w)}+W^{(c)}$ as the low-dimensional representation.

  \textbf{Alternative approaches}, with similar approaches -- first model observation on how co-occurrence can capture semantic relationship, then use gradient descent methods to learn parameters.
  \begin{itemize}
    \item CBOW (Continuous Bag of Words) Model -- Predict word from context
    \item GLOVE -- \emph{Ratios of probabilities} can capture \emph{semantic relationships} among terms. E.g, \texttt{solid} co-occurs more with \texttt{ice} than with \texttt{water}; \texttt{steam} co-occurs with \texttt{gas} than with \texttt{ice}; \texttt{water} co-occurs with both $\Rightarrow$ captures semantics on the meaning of solid and gas, beyond co-occurrence.
  \end{itemize}

  \textbf{Properties} of word embeddings:
  \begin{itemize}
    \item Cluster similar terms (i.e. group similar words together)
    \item Capture \emph{syntactic} relationships (singular-plural, comparative-superlative...)
    \item Capture \emph{semantic} relationships (enable \emph{word analogies} as linear mappings, e.g.  king - man + woman = queen)
    \item Capture \emph{semantic meaning in dimensions}
  \end{itemize}

  \textbf{Use of word embedding models}: Document search (word embedding vectors as document representation); Thesaurus construction and taxonomy induction (search engine for semantically analogous or related terms); Document classification (word embedding vectors of document terms as features).
% subsection word_embeddings (end)