{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two techniques that are part of the so-called shopping basket analysis, which will help us to better understand how customers data are being processed to extract insights about their habits.\n",
    "\n",
    "\n",
    "#### Notes about external libraries\n",
    "You can check your implementation of the Apriori algorithm and the Association Rules using the data mining library MLxtend (`pip install mlxtend`). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "\n",
    "Use the Apriori algorithm to extract frequent itemsets from a list of grocery store transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(dataset):\n",
    "    \"\"\"Formats the transaction dataset.\n",
    "    Expects an array of transactions,\n",
    "    with each transaction being an array of items\n",
    "    \n",
    "    Example:\n",
    "        [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5]]\n",
    "    \"\"\"\n",
    "    # Get frozensets of unique items to use itemsets as dict key\n",
    "    unique_items = list(set([frozenset([item]) for\n",
    "        trans in dataset for item in trans]))\n",
    "    return unique_items, list(map(set,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(Lk):\n",
    "    \"\"\"Generates candidates of length k+1 from a list Lk\n",
    "    of items, each item of length k\n",
    "\n",
    "    Example:\n",
    "        [{1}, {2}, {5}]          -> [{1, 2}, {1, 5}, {2, 5}]\n",
    "        [{2, 3}, {2, 5}, {3, 5}] -> [{2, 3, 5}]\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    # We generate rules of the target size k\n",
    "    k = len(Lk[0])\n",
    "    \n",
    "    for i in range(len(Lk)):\n",
    "        for j in range(i + 1, len(Lk)): \n",
    "            L1 = list(Lk[i])[:k - 1]; \n",
    "            L2 = list(Lk[j])[:k - 1]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "\n",
    "            # Merge sets if first k-1 elements are equal\n",
    "            # If k=1 generates all possible combinations\n",
    "            if L1 == L2:\n",
    "                output.append(Lk[i] | Lk[j])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_support(support, max_display=10, min_items=1):\n",
    "    \"\"\"Prints the results of the apriori algorithm\"\"\"\n",
    "    print('support\\t itemset')\n",
    "    print('-' * 30)\n",
    "    filt_support = {k:v for k, v in support.items() if len(k) >= min_items}\n",
    "    for s, sup in sorted(filt_support.items(), key=operator.itemgetter(1),\n",
    "        reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup, '\\t', set(s))\n",
    "        \n",
    "def print_support_mx(df, max_display=10, min_items=1):\n",
    "    \"\"\"Prints the results of the apriori algorithm\"\"\"\n",
    "    print('support\\t itemset')\n",
    "    print('-' * 30)\n",
    "    lenrow = df['itemsets'].apply(lambda x: len(x))\n",
    "    df = df[lenrow >= min_items]\n",
    "    df = df.sort_values('support', ascending=False).iloc[:max_display]\n",
    "    for i, row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['support']), '\\t', set(row['itemsets']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm\n",
    "\n",
    "The [Apriori Algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm) identifies frequent combinations of items by extending them to larger and larger itemsets (see the generate_candidates function) as long as they appear sufficiently often in a list of transactions.\n",
    "\n",
    "Compute support for all the candidate itemsets contained in Ck, given the total list of transactions. We already provide the functions to compute candidate itemsets. The support of the itemset $X$ with respect to the list of transactions $T$ is defined as the proportion of transactions $t$ in the dataset which contains the itemset $X$. Support can be computed using the following formula\n",
    "\n",
    "$$\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}$$  \n",
    "\n",
    "After computing the support for each itemset, prune the ones that do not match the minimal specificied support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_support(dataset, Ck, min_support):\n",
    "    \"\"\"\n",
    "    Compute support for each provided itemset by counting # of\n",
    "    occurences in original transactions dataset.\n",
    "\n",
    "    dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "    Ck           : list of itemsets to compute support for. \n",
    "    min_support  : minimum support. Itemsets with support below this threshold\n",
    "                   will be pruned.\n",
    "\n",
    "    output       : list of remaining itemsets, after the pruning step.\n",
    "    support_dict : dictionary containing the support value for each itemset.\n",
    "    \"\"\"\n",
    "\n",
    "    def all_contained(subset, set_):\n",
    "        for i in subset:\n",
    "            if i not in set_:\n",
    "                return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # Compute the supports\n",
    "    supports = {}\n",
    "    entries = len(dataset)\n",
    "    for u in Ck:\n",
    "        count = sum([all_contained(u, s) for s in dataset])\n",
    "        supports[u] = count / entries\n",
    "\n",
    "    output = [k for k, v in supports.items() if v >= min_support]\n",
    "\n",
    "    return output, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apriori(dataset, min_support=0.5):\n",
    "    \"\"\"Runs the apriori algorithm\n",
    "\n",
    "    dataset     : list of transactions\n",
    "    min_support : minimum support. Itemsets with support below this threshold\n",
    "                  will be pruned.\n",
    "    \"\"\"\n",
    "    unique_items, dataset = preprocess(dataset)\n",
    "    L1, supportData = get_support(dataset, unique_items, min_support)\n",
    "    \n",
    "    L = [L1]\n",
    "    k = 0\n",
    "    while True:\n",
    "        Ck = generate_candidates(L[k])\n",
    "        Lk, supK = get_support(dataset, Ck, min_support)\n",
    "        \n",
    "        # Check for itemsets of length k with minimum support\n",
    "        if len(Lk):\n",
    "            supportData.update(supK)\n",
    "            L.append(Lk) \n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'whole milk', 'other vegetables'}\n",
      "0.06 \t {'rolls/buns', 'whole milk'}\n",
      "0.06 \t {'whole milk', 'yogurt'}\n",
      "0.05 \t {'root vegetables', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'other vegetables'}\n",
      "0.04 \t {'yogurt', 'other vegetables'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'tropical fruit', 'whole milk'}\n",
      "0.04 \t {'soda', 'whole milk'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "dataset = [l.strip().split(',') for i, l in enumerate(open('groceries.csv').readlines())]\n",
    "\n",
    "L, support = apriori(dataset, min_support=0.01)\n",
    "print_support(support, 10, min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'whole milk', 'other vegetables'}\n",
      "0.06 \t {'rolls/buns', 'whole milk'}\n",
      "0.06 \t {'yogurt', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'whole milk'}\n",
      "0.05 \t {'root vegetables', 'other vegetables'}\n",
      "0.04 \t {'yogurt', 'other vegetables'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'tropical fruit', 'whole milk'}\n",
      "0.04 \t {'soda', 'whole milk'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori as mx_apriori\n",
    "\n",
    "df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).sum(level=0)\n",
    "frequent_itemsets = mx_apriori(df_dummy, min_support=0.01, use_colnames=True)\n",
    "print_support_mx(frequent_itemsets, 10, min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2\n",
    "\n",
    "Such associations are not necessarily symmetric. Therefore, in the second part, we will use [association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to better understand the directionality of frequent itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_rules(L, supportData, min_confidence=0.7):  \n",
    "    \"\"\"Generates association rules given an array of frequent itemsets\n",
    "    and a level of confidence\n",
    "    \n",
    "    Inputs:\n",
    "        L: itemsets\n",
    "        supportData: dictionary storing itemsets support\n",
    "        min_confidence: rules with confidence under threshold are pruned\n",
    "    \"\"\"\n",
    "\n",
    "    # Rules computed\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over itemsets of length >= 2\n",
    "    for i in range(1, len(L)):\n",
    "        # Iterate over each frequent itemset\n",
    "        for freqSet in L[i]:          \n",
    "            # Check if freqSet has more than 2 elements\n",
    "            if (i+1 > 2):\n",
    "                # recursively generate candidates \n",
    "                rules_from_consequent(freqSet, supportData, rules, min_confidence)\n",
    "                compute_confidence(freqSet, supportData, rules, min_confidence)\n",
    "\n",
    "            # Otherwise\n",
    "            else:\n",
    "                # compute rule confidence\n",
    "                compute_confidence(freqSet, supportData, rules, min_confidence)\n",
    "\n",
    "    return rules   \n",
    "\n",
    "\n",
    "def rules_from_consequent(freqSet, supportData, rules, min_confidence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        freqSet: frequent itemset\n",
    "        supportData: dictionary storing itemsets support\n",
    "        rules: array to store rules\n",
    "        min_confidence: rules with confidence under threshold are pruned\n",
    "    \"\"\"\n",
    "    H = [frozenset([item]) for item in freqSet]\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): \n",
    "        # create new candidates of size n+1\n",
    "        Hmp1 = generate_candidates(H)\n",
    "        Hmp1 = compute_confidence(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "        \n",
    "        #need at least two sets to merge\n",
    "        if (len(Hmp1) > 1):\n",
    "            rules_from_consequent(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "            \n",
    "def print_rules(rules, max_display=10):\n",
    "    \"\"\"Prints the resulting rules\"\"\"\n",
    "    print('confidence\\t rule')\n",
    "    print('-' * 30)\n",
    "    for a, b, sup in sorted(rules, key=lambda x: x[2],\n",
    "        reverse = True)[:max_display]:\n",
    "        print(\"%.2f\" % sup, '\\t', set(a), '->', set(b))\n",
    "\n",
    "def print_rules_mx(df,max_display=10):\n",
    "    \"\"\"Prints the resulting rules\"\"\"\n",
    "    print('confidence\\t rule')\n",
    "    print('-' * 30)\n",
    "    df  = df.sort_values('confidence', ascending=False).iloc[:max_display]\n",
    "    for i, row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['confidence']), '\\t',\n",
    "            set(row['antecedants']), '->',set(row['consequents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compute_confidence`\n",
    "\n",
    "`compute_confidence(...)` computes the confidence for a set of candidate rules H and prunes the rules with a confidence below the threshold. The confidence is given by:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_confidence(freqSet, supportData, rules, min_confidence=0.7):\n",
    "    \"\"\"Computes the confidence for a set of rules and their supports\n",
    "    \n",
    "    Inputs:\n",
    "        freqSet: one frequent itemset of N-element\n",
    "        supportData: dictionary storing itemsets support\n",
    "        rules: array to store rules\n",
    "        min_confidence: rules with less confidence are pruned\n",
    "    \"\"\"\n",
    "    H = [frozenset([item]) for item in freqSet]\n",
    "    prunedH = [] \n",
    "    \n",
    "    for Y in H:\n",
    "        # Compute frequent itemsets\n",
    "        X = \n",
    "        \n",
    "        # Compute support for both terms\n",
    "        support_XuY = \n",
    "        support_X = \n",
    "        # Compute confidence\n",
    "        conf = \n",
    "        \n",
    "        if conf >= min_confidence: \n",
    "            rules.append((X, Y, conf))\n",
    "            prunedH.append(Y)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below (you will have to run the checking code of question 1 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'root vegetables', 'citrus fruit'} -> {'other vegetables'}\n",
      "0.58 \t {'tropical fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'yogurt', 'curd'} -> {'whole milk'}\n",
      "0.57 \t {'butter', 'other vegetables'} -> {'whole milk'}\n",
      "0.57 \t {'tropical fruit', 'root vegetables'} -> {'whole milk'}\n",
      "0.56 \t {'root vegetables', 'yogurt'} -> {'whole milk'}\n",
      "0.55 \t {'domestic eggs', 'other vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'yogurt', 'whipped/sour cream'} -> {'whole milk'}\n",
      "0.52 \t {'rolls/buns', 'root vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'pip fruit', 'other vegetables'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules as mx_association_rules\n",
    "\n",
    "rules_mx = mx_association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print_rules_mx(rules_mx,max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL Twitter Data\n",
    "\n",
    "Now that we have a working implementation, we will apply the Apriori algorithm on a dataset that you should know pretty well by now: EPFL Twitter data. In that scenario, tweets will be considered as transactions and words will be items. Let's see what kind of frequent associations we can discover.\n",
    "\n",
    "The method below cleans the tweets and formats them in the same format as the transactions of the previous exercise. Run the cells and generate the results for both algorithms. What can you observe from the association rules results? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L,support = apriori(documents,min_support = 0.01)\n",
    "print_support(support,20,min_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Pen and Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the following accident and weather data. Each line corresponds to one event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. car_accident rain lightning wind clouds fire\n",
    "2. fire clouds rain lightning wind\n",
    "3. car_accident fire wind\n",
    "4. clouds rain wind\n",
    "5. lightning fire rain clouds  \n",
    "6. clouds wind car_accident  \n",
    "7. rain lightning clouds fire  \n",
    "8. lightning fire car_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) You would like to know what is the likely cause of all the car accidents. What association rules do you need to look for? Compute the confidence and support values for these rules. Looking at these values, which is the most likely cause of the car accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find all the association rules for minimal support 0.6 and minimal confidence of 1.0 (certainty). Follow the apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
