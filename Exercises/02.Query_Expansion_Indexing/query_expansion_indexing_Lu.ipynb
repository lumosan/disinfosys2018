{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Expansion and Indexing\n",
    "\n",
    "0. [Vector Space Retrieval](#Vector-Space-Retrieval)\n",
    "1. [Local approach for extending queries: User Relevance Feedback (Rocchio Algorithm)](#Question-1---Relevance-Feedback)\n",
    "2. [Distributed Information Retrieval (Fagin's Algorithm)](#Question-2----Distributed-Information-Retrieval)\n",
    "3. [Inverted Files (Hard)](#Question-3---Inverted-Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_documents = ['B1 A course on Integral equations',\n",
    "    'B2 Attractors for semigroups and evolution equations',\n",
    "    'B3 Automatic differentiation of algorithms: theory, implementation, and application',\n",
    "    'B4 Geometrical aspects of partial differential equations',\n",
    "    'B5 Ideals, varieties and algorithms: an introduction to computational algebraic geometry and commutative algebra',\n",
    "    'B6 Introduction to Hamiltonian dynamical systems and the N-body problem',\n",
    "    'B7 Knapsack problems: Algorithms and computer implementations',\n",
    "    'B8 Methods of solving singular systems of ordinary differential equations with delay',\n",
    "    'B9 Nonlinear systems',\n",
    "    'B10 Ordinary differential equations',\n",
    "    'B11 Oscillation theory for neutral differential equations with delay',\n",
    "    'B12 Oscillation theory for delay differential equations',\n",
    "    'B13 Pseudodifferential operators and nonlinear partial differential equations',\n",
    "    'B14 Sinc methods for quadrature and differential equations',\n",
    "    'B15 Stability of stochastic differential equations with respect to semi-martingales',\n",
    "    'B16 The boundary integral approach to static and dynamic contact problems',\n",
    "    'B17 The double mellin-barnes type integrals and their applications to convolution theory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Retrieval\n",
    "The following code is modified from the previous lab. It is used to construct the vocabulary and vectorize the documents and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    \"\"\"Given a string, removes the punctuation and returns\n",
    "    array of stemmed words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = \"\".join([ch for ch in text if ch\n",
    "        not in string.punctuation])\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Return stemmed document\n",
    "    return \" \".join([stemmer.stem(word.lower())\n",
    "        for word in tokens])\n",
    "\n",
    "\n",
    "def read_documents(filename):\n",
    "    # Read documents from file (each line is a document)\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    original_documents = [x.strip() for x in content] \n",
    "    documents = [tokenize(d).split() for d in original_documents]\n",
    "    return documents, original_documents\n",
    "\n",
    "\n",
    "def create_vocabulary(documents):\n",
    "    # Create the vocabulary\n",
    "    # flatten 'documents'\n",
    "    vocabulary = set([item for sublist in documents\n",
    "        for item in sublist])\n",
    "    # remove stopwords and sort\n",
    "    vocabulary = [word for word in vocabulary if\n",
    "        word not in stopwords.words('english')]\n",
    "    vocabulary.sort()\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# Compute IDF, storing values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"Computes IDF as log(N/nt)\n",
    "    where nt is the number of documents where the\n",
    "    term t appears.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for term in vocabulary:\n",
    "        idf[term] = [term in x for x in documents]\n",
    "        idf[term] = math.log(num_documents /\n",
    "            Counter(idf[term])[True], math.e)\n",
    "    return idf\n",
    "\n",
    "\n",
    "# Generate the vector for a document (with normalization)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"Returns TF-IDF as TF*IDF\n",
    "    Computes TF as the term frequency, which means\n",
    "    ftd/max_count\n",
    "    where:\n",
    "        ftd = nb. of times term t occurs in doc d\n",
    "        max_count = nb. of times most common term occurs in d\n",
    "    This is known as 'augmented frequency' and prevents\n",
    "    a bias towards longer documents.\n",
    "    \"\"\"\n",
    "    tfidf = [0] * len(vocabulary)\n",
    "    tf = [0] * len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    # get the count for most common word\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        tf[i] = counts[term] / max_count \n",
    "        tfidf[i] = tf[i] * idf[term]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]\n",
    "        y = v2[i]\n",
    "        sumxx += x * x\n",
    "        sumyy += y * y\n",
    "        sumxy += x * y\n",
    "    if sumxy == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sumxy / math.sqrt(sumxx * sumyy)\n",
    "    \n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    # get array of vectorized query words\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    # get the vector for the query\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "   \n",
    "    \n",
    "# Compute the search result (get topk documents)\n",
    "def search_vec(query, topk):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    # compute cosine similarity scores\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d]\n",
    "        for d in range(len(documents))]\n",
    "    # sort and return search result\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(topk):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "        \n",
    "# Put everything together\n",
    "def search_scores_vec(query, topk, vocabulary, documents,\n",
    "    original_documents, idf=None, document_vectors=None):\n",
    "\n",
    "    def search_vec_2(query, topk, vocabulary, idf, document_vectors,\n",
    "        original_documents):\n",
    "        query_vector = vectorize_query(query, vocabulary, idf)\n",
    "        # compute cosine similarity scores\n",
    "        scores = [[cosine_similarity(query_vector, document_vectors[d]), d]\n",
    "            for d in range(len(documents))]\n",
    "        # sort and return search result\n",
    "        scores.sort(key=lambda x: -x[0])\n",
    "        ans = []\n",
    "        indices = []\n",
    "        for i in range(topk):\n",
    "            ans.append(original_documents[scores[i][1]])\n",
    "            indices.append(scores[i][1])\n",
    "        return ans, indices, query_vector\n",
    "    \n",
    "    if idf is None:\n",
    "        idf = idf_values(vocabulary, documents)\n",
    "    if document_vectors is None:\n",
    "        document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "    ans, indices, query_vector = search_vec_2(query, topk, vocabulary,\n",
    "        idf, document_vectors, original_documents)\n",
    "    for d in ans:\n",
    "        print(d)\n",
    "    return ans, indices, query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Relevance Feedback\n",
    "\n",
    "In this exercise we will implement and test Rocchio's method for user relevance feedback.\n",
    "\n",
    "Its objective is to find a query $\\vec{q}_{opt}$ that optimally separates relevant ($D_r$) and non-relevant ($D_{nr}$) documents:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q}_{opt} = \\text{arg max}_{\\vec{q}}[\\text{sim}(\\vec{q}, \\mu(D_r))-\\text{sim}(\\vec{q}, \\mu(D_{nr}))]\n",
    "\\end{equation}\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_k} \\in D_{nr}} \\vec{d_k}\n",
    "\\end{equation}\n",
    "\n",
    "With:\n",
    "\n",
    "| Variable    | Value                       |\n",
    "|-------------|-----------------------------|\n",
    "| $\\vec{q_m}$ | Modified Query Vector       |\n",
    "| $\\vec{q_0}$ | Original Query Vector       |\n",
    "| $\\vec{d_j}$ | Related Document Vector     |\n",
    "| $\\vec{d_k}$ | Non-Related Document Vector |\n",
    "| $\\alpha$    | Original Query Weight       |\n",
    "| $\\beta$     | Related Documents Weight    |\n",
    "| $\\gamma$    | Non-Related Documents Weight|\n",
    "| $D_r$       | Set of Related Documents    |\n",
    "| $D_{nr}$    | Set of Non-Related Documents|\n",
    "\n",
    "In the Rocchio algorithm negative term weights are ignored. This means that the negative term weights in $\\vec{q_m}$ are set to 0.\n",
    "\n",
    "First, complete the implementation of the Rocchio relevance feedback method, by adding the missing code for the function $expand\\_query$.   \n",
    "\n",
    "Then, compare the result obtained with the new query to the unmodified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs,\n",
    "    query_vector, alpha, beta, gamma):\n",
    "    \n",
    "    relevant_doc_vecs = np.array(relevant_doc_vecs)\n",
    "    non_relevant_doc_vecs = np.array(non_relevant_doc_vecs)\n",
    "    query_vector = np.array(query_vector)\n",
    "    'Dogs are predators and scavengers and the dog has powerful muscles'\n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = alpha * query_vector\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = (beta / num_rel *\n",
    "        relevant_doc_vecs.sum(axis=0))\n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = (gamma / num_non_rel *\n",
    "        non_relevant_doc_vecs.sum(axis=0))\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = (norm_query_vector +\n",
    "        norm_sum_relevant - norm_sum_non_relevant)\n",
    "    \n",
    "    # Ignore negative elements\n",
    "    modified_query_vector = np.maximum(modified_query_vector,\n",
    "        np.zeros(len(query_vector)))\n",
    "\n",
    "    return modified_query_vector.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will gets the results for queries \"baking\" and \"computer science\", respectively:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read documents\n",
    "documents, original_documents = read_documents(\"bread.txt\")\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(documents)\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "topk = 3\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "ans, result_doc_ids, query_vector = search_scores_vec(\"recipe\",\n",
    "    topk, vocabulary, documents, original_documents,\n",
    "    idf=idf, document_vectors=document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3 Automatic differentiation of algorithms: theory, implementation, and application\n",
      "B17 The double mellin-barnes type integrals and their applications to convolution theory\n",
      "B12 Oscillation theory for delay differential equations\n",
      "B11 Oscillation theory for neutral differential equations with delay\n"
     ]
    }
   ],
   "source": [
    "original_documents = class_documents\n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(documents)\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "topk = 4\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "ans, result_doc_ids, query_vector = search_scores_vec(\"application theory\",\n",
    "    topk, vocabulary, documents, original_documents,\n",
    "    idf=idf, document_vectors=document_vectors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Read documents\n",
    "documents, original_documents = read_documents(\"epfldocs.txt\")\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(documents)\n",
    "# Compute IDF values and vectors\n",
    "topk = 10\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "ans, result_doc_ids, query_vector = search_scores_vec(\"computer science\",\n",
    "    topk, vocabulary, documents, original_documents,\n",
    "    idf=idf, document_vectors=document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces the result for the unmodified query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unmodified_query(relevant_indices, ans, result_doc_ids, query_vector,\n",
    "    alpha, beta, gamma, document_vectors, vocabulary):\n",
    "    \n",
    "    def items_from_list(indices, alist):\n",
    "        items = itemgetter(*indices)(alist)\n",
    "        if type(items) == tuple:\n",
    "            return list(items)\n",
    "        else:\n",
    "            return [items]\n",
    "        \n",
    "    # remove relevant indices from all indices\n",
    "    non_relevant_indices =  [n for n in range(len(ans)) if n not in relevant_indices]\n",
    "    # get indices of relevant and non-relevant results\n",
    "    relevant_doc_ids = items_from_list(relevant_indices, result_doc_ids)\n",
    "    non_relevant_doc_ids = items_from_list(non_relevant_indices, result_doc_ids)\n",
    "    # get vectors of relevant and non-relevant results\n",
    "    relevant_doc_vecs = items_from_list(relevant_doc_ids, document_vectors)\n",
    "    non_relevant_doc_vecs = items_from_list(non_relevant_doc_ids, document_vectors)\n",
    "    # expand query\n",
    "    expanded_query = expand_query(relevant_doc_vecs,\n",
    "        non_relevant_doc_vecs, query_vector, alpha, beta, gamma)\n",
    "    # obtain new query vocabulary\n",
    "    new_query = ' '.join([vocabulary[i] for i, val in\n",
    "        enumerate(expanded_query) if val > 0])\n",
    "    \n",
    "    return new_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# list of indices marked as relevant\n",
    "relevant_indices = [1]\n",
    "\n",
    "# obtain new query vocabulary\n",
    "new_query = get_unmodified_query(relevant_indices,\n",
    "    ans, result_doc_ids, query_vector, 1, .75, .25,\n",
    "    document_vectors, vocabulary)\n",
    "\n",
    "# obtain answer for new query\n",
    "new_ans, new_result_doc_ids, new_query_vector = search_vec(new_query, 3)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  algorithm applic automat b3 differenti implement theori\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B3 Automatic differentiation of algorithms: theory, implementation, and application',\n",
       " 'B7 Knapsack problems: Algorithms and computer implementations',\n",
       " 'B17 The double mellin-barnes type integrals and their applications to convolution theory',\n",
       " 'B12 Oscillation theory for delay differential equations']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "relevant_indices = [0]\n",
    "\n",
    "# obtain new query vocabulary\n",
    "new_query = get_unmodified_query(relevant_indices,\n",
    "    ans, result_doc_ids, query_vector, 1, .75, .25,\n",
    "    document_vectors, vocabulary)\n",
    "\n",
    "# obtain answer for new query\n",
    "new_ans, new_result_doc_ids, new_query_vector = search_vec(new_query, 4)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# list of indices marked as relevant\n",
    "relevant_indices = [1, 2]\n",
    "\n",
    "# obtain new query vocabulary\n",
    "new_query = get_unmodified_query(relevant_indices,\n",
    "    ans, result_doc_ids, query_vector, 1, 1, 1,\n",
    "    document_vectors, vocabulary)\n",
    "\n",
    "# obtain answer for new query\n",
    "new_ans, new_result_doc_ids, new_query_vector = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 -  Distributed Information Retrieval\n",
    "\n",
    "In this exercise we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists. Assume an aggregation function that returns the sum of the TF-IDF scores given the terms in a document.\n",
    "\n",
    "We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "\n",
    "We first create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for term, assumed to be hosted in a different node. Explore the structure of $h$, to understand it. We suggest to first use the simple collection breads.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_h(document_vectors, vocabulary, original_documents):\n",
    "    doc_vecs = np.array(document_vectors)\n",
    "    h = {}\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        ht = {}\n",
    "        for docj in range(len(original_documents)):\n",
    "            tfidf = doc_vecs[docj][i]\n",
    "            ht[docj] = tfidf\n",
    "        # sort by higher tf-idf\n",
    "        sorted_ht = sorted(ht.items(), key=itemgetter(1), reverse=True)\n",
    "        h[term] = sorted_ht\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the following function that implements the Fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fagin_algorithm(query, h, k, vocabulary): \n",
    "    def prepare_vbles(query, h):\n",
    "        # Split and stem the query\n",
    "        q = query.split()\n",
    "        q = set([stemmer.stem(w) for w in q])\n",
    "        query_term_cnt = len(q)\n",
    "    \n",
    "        # Select the posting lists for the query terms\n",
    "        posting_lists = {}\n",
    "        for term in q:\n",
    "            if term in h:\n",
    "                posting_lists[term] = h[term]\n",
    "                \n",
    "        # Count documents\n",
    "        max_len = len(h[list(h.keys())[0]])\n",
    "\n",
    "        return q, query_term_cnt, posting_lists, max_len\n",
    "    \n",
    "    def phase_1_Fagin(posting_lists, k, query_term_cnt):\n",
    "        \"\"\"Phase 1 of Fagin's algorithm\n",
    "        Traverses the selected posting lists until finding k documents\n",
    "        that appear in all posting lists, then stops.\n",
    "\n",
    "        The result is a dictionary documents_occurrences, with:\n",
    "            document identifiers as keys, \n",
    "            number of documents found as value.\n",
    "        \"\"\"        \n",
    "        x = list(posting_lists.values())\n",
    "        documents_occurrences = {}\n",
    "        for j in range(max_len):\n",
    "            for i in x:\n",
    "                (idx, value) = i[j]\n",
    "                if idx in documents_occurrences:\n",
    "                    documents_occurrences[idx] = documents_occurrences[idx] + 1\n",
    "                else:\n",
    "                    documents_occurrences[idx] = 1\n",
    "                    \n",
    "                if list(documents_occurrences.values()).count(query_term_cnt) >= k:\n",
    "                    break\n",
    "            if list(documents_occurrences.values()).count(query_term_cnt) >= k:\n",
    "                break\n",
    "        print(documents_occurrences)\n",
    "        return documents_occurrences\n",
    "\n",
    "\n",
    "    def phase_2_Fagin(posting_lists, documents_occurrences, q):\n",
    "        \"\"\"Retrieve for the found documents the tfidf values and add them up.\n",
    "        For simplicity, do not distinguish the cases when\n",
    "            the values have already been retrieved in the previous phase, or\n",
    "            we use random access to obtain those values\n",
    "        \"\"\"\n",
    "        tfidf = {}\n",
    "        for d in documents_occurrences.keys():\n",
    "            t = 0\n",
    "            for term in q:\n",
    "                t = t + dict(posting_lists[term])[d]\n",
    "            tfidf[d] = t\n",
    "        return tfidf\n",
    "        \n",
    "    q, query_term_cnt, posting_lists, max_len = prepare_vbles(query, h)\n",
    "    documents_occurrences = phase_1_Fagin(posting_lists, k, query_term_cnt)\n",
    "    tfidf = phase_2_Fagin(posting_lists, documents_occurrences, q)\n",
    "\n",
    "    # Lastly, compute the top-k documents and return the answer\n",
    "    ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 2, 2: 1, 0: 2, 1: 1}\n",
      "[(3, 1.1394342831883648), (0, 0.56971714159418241)]\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "How to Bake Breads Without Baking Recipes\n"
     ]
    }
   ],
   "source": [
    "ans = fagin_algorithm(\"bread recipe\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])\n",
    "\n",
    "# Provided solution:\n",
    "#{3: 2, 2: 1, 0: 2, 1: 1}\n",
    "#[(3, 1.1394342831883648), (0, 0.56971714159418241)]\n",
    "#Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "#How to Bake Breads Without Baking Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce your own datasets to explore the behavior of the algorithm. Create a dataset such that for retrieving a 2 term query a total of 6 documents needs to be retrieved in the first phase of the algorithm (as in the example in the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 3: 2, 1: 1, 4: 1, 2: 1, 5: 1}\n",
      "[(0, 0.54062014414421911), (3, 0.54062014414421911)]\n",
      "Dog breeds are obtained by mating selected dogs to maintain dog characteristics\n",
      "Dog breeds are lame but cat breeds and hedgehog breeds are even worse\n"
     ]
    }
   ],
   "source": [
    "original_documents = ['Dog breeds are obtained by mating selected dogs to maintain dog characteristics',\n",
    "    'Dogs are predators and scavengers and the dog has powerful muscles',\n",
    "    'My cousin was bitten by a dog and now hates dogs',\n",
    "    'Dog breeds are lame but cat breeds and hedgehog breeds are even worse',\n",
    "    'Breeding animals of one breed with animals of another breed are mixed breeds',\n",
    "    'Plant breeds are obtained by breeding plants']\n",
    "\n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(documents)\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "\n",
    "h = create_dictionary_h(document_vectors, vocabulary, original_documents)\n",
    "\n",
    "ans = fagin_algorithm(\"dog breeding\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Inverted Files\n",
    "\n",
    "An inverted list $l_k$ contains the number of documents in which term $k$ occurs ($f_k$) and the list of document identifiers containing $k$ ($d_{i1}$ ... $d_{if_k}$)\n",
    "\n",
    "Inverted files are built by concatenating the inverted lists for all terms occurring in the document collection:\n",
    "\n",
    "Now assume an inverted list that stores not only the document identifiers of the documents in which the term appears, but also an *offset* of the appearance of a term in a document.\n",
    "\n",
    "An *offset* of a term $l_k$ given a document is defined as the number of words between the start of the document and $l_k$. Thus our inverted list is now:\n",
    "\n",
    "$l_k= \\langle f_k: \\{d_{i_1} \\rightarrow [o_1,\\ldots,o_{n_{i_1}}]\\}, \n",
    "\\{d_{i_2} \\rightarrow [o_1,\\ldots,o_{n_{i_2}}]\\}, \\ldots, \n",
    "\\{d_{i_k} \\rightarrow [o_1,\\ldots,o_{n_{i_k}}]\\} \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This means that in document $d_{i_1}$ term $k$ appears $n_{i_1}$ times and at offset $[o_1,,o_{n_{i_1}}]$, where $[o_1,,o_{n_{i_1}}]$ are sorted in ascending order, these type of indices are also known as term-offset indices. An example of a term-offset index is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obama** = $⟨4 : {1 → [3]},{2 → [6]},{3 → [2,17]},{4 → [1]}⟩$\n",
    "\n",
    "**Governor** = $⟨2 : {4 → [3]}, {7 → [14]}⟩$\n",
    "\n",
    "**Election** = $⟨4 : {1 → [1]},{2 → [1,21]},{3 → [3]},{5 → [16,22,51]}⟩$\n",
    "\n",
    "Which is to say that the term **Governor** appear in 2 documents. In document 4 at offset 3, in document 7 at offset 14.\n",
    "\n",
    "Now let us consider the _**SLOP/x**_ operator in text retrieval, which has the syntax: *QueryTerm1 SLOP/x QueryTerm2* (with $x$ an integer positive number $x \\geq 1$).\n",
    "\n",
    "It finds occurrences of *QueryTerm1* within $x$ (but not necessarily in that order) words of *QueryTerm2*. Thus $x = 1$ demands that *QueryTerm1* be adjacent to *QueryTerm2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lk = {\"obama\":(4, {1:[3], 2:[6], 3:[2,17], 4:[1]}),\n",
    "      \"governor\":(2, {4:[3], 7:[14]}),\n",
    "      \"election\":(4, {1:[1], 2:[1,21], 3:[3], 5:[16,22,51]})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slop_x(query_term_1, query_term_2, x):\n",
    "    docs_found = []\n",
    "    for d in lk[query_term_1][1]:\n",
    "        try:\n",
    "            q2 = lk[query_term_2][1][d]\n",
    "            for o1 in lk[query_term_1][1][d]:\n",
    "                for o2 in q2:\n",
    "                    if d in docs_found:\n",
    "                        break\n",
    "                    if abs(o1 - o2) == x:\n",
    "                        docs_found.append(d)\n",
    "                        break\n",
    "        except KeyError:\n",
    "            pass      \n",
    "\n",
    "    return docs_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1 List the set of documents that satisfy the query **Obama _SLOP/2_ Election**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2\n",
    "query_term_1 = \"obama\"\n",
    "query_term_2 = \"election\"\n",
    "slop_x(query_term_1, query_term_2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 List each set of values for which the query **Obama _SLOP/x_ Election** has a different set of documents as answers (starting from $x = 1$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [3], 2: [1], 5: [2], 14: [3], 15: [2]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_term_1 = \"obama\"\n",
    "query_term_2 = \"election\"\n",
    "answers = {}\n",
    "for x in range(52):\n",
    "    ans = slop_x(query_term_1, query_term_2, x)\n",
    "    if len(ans) > 0:\n",
    "        answers[x] = ans\n",
    "\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3 Consider the general procedure for \"merging\" two term-offset inverted lists for a given document, to determine where the document satisfies a *SLOP/x* clause (since in general there will be many offsets at which each term occurs in a document). Let $L$ denote the total number of occurrences of the two terms in the document. Assume we have a pointer to the list of occurrences of each term and can move the pointer along this list. As we do so we check whether we have a hit for *SLOP/x* (i.e. the *SLOP/x* clause is satisfied). Each move of either pointer counts as a step. Based on this assumption is there a general \"merging\" procedure to determine whether the document satisfies a *SLOP/x* clause, for which the following is true? Justify your answer.\n",
    "\n",
    "1. The merge can be accomplished in a number of steps linear in $L$ regardless of $x$, and we can ensure that each pointer moves only to the right (i.e. forward).\n",
    "2. The merge can be accomplished in a number of steps linear in $L$, but a pointer may be forced to move to the left (i.e. backwards).\n",
    "3. The merge can require $x \\times L$ steps in some cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
