{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Advanced Information Retrieval Solutions\n",
    "\n",
    "##  Question 1 - Latent Semantic Indexing\n",
    "\n",
    "###  Question 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.3359611 ,  0.1962311 , -0.25246121,  0.11968319],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.11909604,  0.2663899 ,  0.20432237, -0.52093504],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.39922386, -0.49767812, -0.57172873,  0.04465203],\n",
       "       [-0.41291701, -0.12294407,  0.05933248, -0.03660797],\n",
       "       [-0.30751414, -0.01459992,  0.48607132,  0.40306708],\n",
       "       [-0.07695592, -0.31917516,  0.31179369, -0.15629115],\n",
       "       [-0.45505713,  0.462621  , -0.04813884, -0.40125186],\n",
       "       [-0.23055822,  0.30457524,  0.17427762,  0.55935823]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Python matrix operations library\n",
    "import numpy as np\n",
    "\n",
    "#set M matrix using the given values.\n",
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "\n",
    "M = np.array(M)\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "# Print K. **(note that values can match upto a sign + or -.)**\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.78695453, 2.31848919, 1.762346  , 0.77705263])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36838448, -0.57010731, -0.53356439, -0.50455879],\n",
       "       [-0.74000417,  0.61762211,  0.0885323 , -0.25119473],\n",
       "       [ 0.54948837,  0.36008671, -0.05294924, -0.75206148],\n",
       "       [-0.12144645, -0.40479395,  0.83944473, -0.34165065]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  Question 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select 2 singular values.\n",
    "\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = np.diag(S)[0:2,0:2]\n",
    "Dt_sel = Dt[0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22662409,  0.11624731])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform query.\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "\n",
    "#Map the query q onto the document space D as q* = qT · (K_sel · S_sel−1)\n",
    "mapper = np.dot(K_sel, np.linalg.inv(S_sel))\n",
    "q_trans =  np.dot( q, mapper)\n",
    "\n",
    "# Check q_trans\n",
    "q_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1: -0.012057913278690218  d2: 0.9388827727147443  d3: 0.9524776244205609 d4 0.5931086268074783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity.\n",
    "    \n",
    "import math\n",
    "\n",
    "# Function for computing cosine similarity.\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "\n",
    "# Now extract representations of documents in the new concept space.\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d3 = Dt_sel[:,2]\n",
    "d4 = Dt_sel[:,3]\n",
    "\n",
    "\n",
    "# compute cosine similarity.\n",
    "sim_d1 = cosine_similarity(d1, q_trans)\n",
    "sim_d2 = cosine_similarity(d2, q_trans)\n",
    "sim_d3 = cosine_similarity(d3, q_trans)\n",
    "sim_d4 = cosine_similarity(d4, q_trans)\n",
    "\n",
    "\n",
    "#print Similarities\n",
    "print(\"d1: {}  d2: {}  d3: {} d4 {} \\n\".format(sim_d1, sim_d2, sim_d3, sim_d4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ordering of documents:  D3 > D2 > D4 > D1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  Question 1.e\n",
    "\n",
    "The document ordering does not change even if d3 is dropped. Recall that all the documents in the term-document matrix can be considered as vectors in a $R^m$ dimensional vector space. Thus, since d3 has a similar magnitude and direction as d4 and d2, dropping d3 does not alter substantially the term space ( K ) and the document space ( D ) of the SVD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1: 0.10125520472871084  d2: 0.9475215481378308  d4 0.6873076021729543 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mn = [[1,1,1],\n",
    "      [0,1,1],\n",
    "      [1,0,0],\n",
    "      [0,1,0],\n",
    "      [1,0,0],\n",
    "      [1,0,2],\n",
    "      [1,1,1],\n",
    "      [1,1,0],\n",
    "      [1,0,0],\n",
    "      [0,2,1],\n",
    "      [0,1,0]]\n",
    "\n",
    "Mn = np.array(Mn)\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(Mn, full_matrices=False)\n",
    "\n",
    "# LSI select dimensions\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = np.diag(S)[0:2,0:2]\n",
    "Dt_sel = Dt[0:2,:]\n",
    "\n",
    "# transform query and documents\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "q_trans =  np.dot( np.dot(q, K_sel), np.linalg.inv(S_sel))\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d4 = Dt_sel[:,2]\n",
    "\n",
    "# compute cosine similarity.\n",
    "sim_d1 = cosine_similarity(d1, q_trans)\n",
    "sim_d2 = cosine_similarity(d2, q_trans)\n",
    "sim_d4 = cosine_similarity(d4, q_trans)\n",
    "\n",
    "\n",
    "#print Similarities\n",
    "print(\"d1: {}  d2: {}  d4 {} \\n\".format(sim_d1, sim_d2, sim_d4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the term and document space we should change d3 such that it in a different direction as compared to the other vectors. For example, d3 = (0, 0, 1, 1, 2, 1, 0, 0, 2, 0, 2) changes the document ordering to d2 >d4 >d1 >d3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1: 0.8574859998903472  d2: 0.9083433060184866 d3: 0.3170877874947522  d4 0.9049563233586899 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mn = [[1,1,0,1],\n",
    "      [0,1,0,1],\n",
    "      [1,0,1,0],\n",
    "      [0,1,1,0],\n",
    "      [1,0,2,0],\n",
    "      [1,0,1,2],\n",
    "      [1,1,0,1],\n",
    "      [1,1,0,0],\n",
    "      [1,0,2,0],\n",
    "      [0,2,0,1],\n",
    "      [0,1,2,0]]\n",
    "\n",
    "\n",
    "Mn = np.array(Mn)\n",
    "\n",
    "# compute SVD\n",
    "K, S, Dt = np.linalg.svd(Mn, full_matrices=False)\n",
    "\n",
    "# LSI select dimensions\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = np.diag(S)[0:2,0:2]\n",
    "Dt_sel = Dt[0:2,:]\n",
    "\n",
    "# transform query and documents\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "q_trans =  np.dot( np.dot(q, K_sel), np.linalg.inv(S_sel))\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d3 = Dt_sel[:,2]\n",
    "d4 = Dt_sel[:,3]\n",
    "\n",
    "# compute cosine similarity.\n",
    "sim_d1 = cosine_similarity(d1, q_trans)\n",
    "sim_d2 = cosine_similarity(d2, q_trans)\n",
    "sim_d3 = cosine_similarity(d3, q_trans)\n",
    "sim_d4 = cosine_similarity(d4, q_trans)\n",
    "\n",
    "\n",
    "#print Similarities\n",
    "print(\"d1: {}  d2: {} d3: {}  d4 {} \\n\".format(sim_d1, sim_d2, sim_d3, sim_d4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic Interpretation:  \n",
    "\n",
    "recall that the matrix M transforms a unit ball into an ellipsoid, and in LSI we keep only the directions with the strongest distortion. Intuitively, if we combine linearly $d_2$ and $d_4$ with a 0.5 coefficient, we’ll find a vector that is not very dissimilar from $d_3$ (i.e., the norm is almost the same, and the direction overlaps on many components). Therefore, it’s not surprising that (in this specific example) removing $d_3$ did not lead to a different ranking. Bear in mind that, with slightly different numbers, this might not be the case anymore.\n",
    "In a real-world scenario with LSI (i.e., millions of documents) removing just a few documents rarely changes the ranking dramatically, because the documents we still keep into account will have high probability to contain the same concepts that are contained in the removed ones. That is to say, the resulting ellipsoid won’t change substantially.\n",
    "\n",
    "\n",
    "###  Question 1.f\n",
    "\n",
    "Use the code from previous exercise:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'bake',\n",
       " 'best',\n",
       " 'book',\n",
       " 'bread',\n",
       " 'cake',\n",
       " 'comput',\n",
       " 'french',\n",
       " 'london',\n",
       " 'numer',\n",
       " 'pastri',\n",
       " 'pie',\n",
       " 'quantiti',\n",
       " 'recip',\n",
       " 'scientif',\n",
       " 'smith',\n",
       " 'without']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1: 0.9980518678772611  d2: -0.6577609566355869 d3: -0.00232887505296717  d4 0.7231078789682557 d5 -0.6551062911361925 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## take transpose of document vectors to convert to term document matrix.\n",
    "M = np.matrix.transpose(np.array(document_vectors))\n",
    "\n",
    "\n",
    "## Run LSI.\n",
    "K, S, Dt = np.linalg.svd(M, full_matrices=False)\n",
    "K_sel = K[:,0:3]\n",
    "S_sel = np.diag(S)[0:3,0:3]\n",
    "Dt_sel = Dt[0:3,:]\n",
    "\n",
    "\n",
    "# transform query and documents\n",
    "q = np.array([0]*len(vocabulary))\n",
    "\n",
    "#Set the term corresponding to baking = 1 (see vocabulary)\n",
    "q[1] = 1\n",
    "q_trans =  np.dot( np.dot(q, K_sel), np.linalg.inv(S_sel))\n",
    "\n",
    "\n",
    "# Now extract representations of documents in the new concept space.\n",
    "d1 = Dt_sel[:,0]\n",
    "d2 = Dt_sel[:,1]\n",
    "d3 = Dt_sel[:,2]\n",
    "d4 = Dt_sel[:,3]\n",
    "d5 = Dt_sel[:,4]\n",
    "\n",
    "\n",
    "# compute cosine similarity.\n",
    "sim_d1 = cosine_similarity(d1, q_trans)\n",
    "sim_d2 = cosine_similarity(d2, q_trans)\n",
    "sim_d3 = cosine_similarity(d3, q_trans)\n",
    "sim_d4 = cosine_similarity(d4, q_trans)\n",
    "sim_d5 = cosine_similarity(d5, q_trans)\n",
    "\n",
    "\n",
    "#print Similarities\n",
    "print(\"d1: {}  d2: {} d3: {}  d4 {} d5 {} \\n\".format(sim_d1, sim_d2, sim_d3, sim_d4, sim_d5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Word Embeddings\n",
    "\n",
    "###  Question 2.a\n",
    "\n",
    "See the attached images: EPFL_embeddings_1/2/3. The terms that are similar are shown in red rectangles. These examples include: French propositions, English propositions, scientists/researchers, EPFL.\n",
    "\n",
    "\n",
    "### Question 2.b\n",
    "\n",
    "The code for finding the closest terms is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings('fastText-0.1.0/model_epfldocs.vec')\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=3):\n",
    "    term_embeddings_dict = {}\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        term_embeddings_dict[term] = word_embeddings[i]\n",
    "        \n",
    "    if input_term not in term_embeddings_dict:\n",
    "        return \"Term not in the vocabulary\"\n",
    "    \n",
    "    input_term_embedding = term_embeddings_dict[input_term]\n",
    "    term_similarities = []\n",
    "    for term, embedding in term_embeddings_dict.items():\n",
    "        term_similarities.append([term, cosine_similarity(input_term_embedding, embedding)])\n",
    "        \n",
    "    sorted_terms = sorted(term_similarities, key = lambda x: -1 * x[1])[0:num_terms]\n",
    "    \n",
    "    return sorted_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['la', 1.0],\n",
       " ['pour', 0.9995999844122386],\n",
       " ['les', 0.9994841513525654],\n",
       " ['sur', 0.9994440687398943],\n",
       " ['faire', 0.9994296343122635]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar('la', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EPFL', 1.0],\n",
       " ['@EPFL', 0.9998930356734843],\n",
       " ['#EPFL', 0.9998703686783511],\n",
       " ['@CHUVLausanne', 0.9998271224201326],\n",
       " ['Lausanne', 0.9998068187511153]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar('EPFL', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#robot', 1.0],\n",
       " ['#robots', 0.9999717434996156],\n",
       " ['#robotics', 0.9999529527060248],\n",
       " ['robot', 0.999944298108067],\n",
       " ['#Robotics', 0.9998920366384801]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar('#robot', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 1.0],\n",
       " ['these', 0.9998446875714924],\n",
       " ['that', 0.9998405424688396],\n",
       " ['there', 0.9998339926559185],\n",
       " ['their', 0.9998318984301732]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar('this', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c\n",
    "\n",
    "Left as exercise.\n",
    "\n",
    "###  Question 2.d\n",
    "\n",
    "See the attached images: Full_embeddings_1/2. There are many different clusters of meaningful concepts such as people names, countries, cities, diseases, websites, vehicles etc. Multiple different patterns can be found in the visualization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
