{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector Space Retrieval\n",
    "\n",
    "1. [Vector space retrieval: TD_IDF implementation](#Exercise-1)\n",
    "2. [Probabilistic retrieval model](#Exercise-2)\n",
    "3. [Theory exercises](#Exercise-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise we will understand the functioning of TF-IDF ranking by implementing the vector space retrieval model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia:\n",
    "\n",
    "*TF-IDF (term frequency-inverse document frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. Its value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.*\n",
    "\n",
    "*TF-IDF can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.*\n",
    "\n",
    "*One of the simplest ranking functions is computed by summing the TF-IDF for each query term; many more sophisticated ranking functions are variants of this simple model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing we have provided a simple document collection with 5 documents in file bread.txt:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Your task is to find the top ranked documents according to the TF-IDF rank for the query \n",
    "$Q$ = `\"baking\"`\n",
    "\n",
    "For further testing, use the collection __epfldocs.txt__, which contains recent tweets mentioning EPFL.\n",
    "\n",
    "Compare the results also to the results obtained from the reference implementation using the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lucia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    \"\"\"Given a string, removes the punctuation and returns\n",
    "    array of stemmed words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = \"\".join([ch for ch in text if ch\n",
    "        not in string.punctuation])\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Return stemmed document\n",
    "    return \" \".join([stemmer.stem(word.lower())\n",
    "        for word in tokens])\n",
    "\n",
    "\n",
    "def read_documents(filename):\n",
    "    # Read documents from file (each line is a document)\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    original_documents = [x.strip() for x in content] \n",
    "    documents = [tokenize(d).split() for d in original_documents]\n",
    "    return documents, original_documents\n",
    "\n",
    "\n",
    "def create_vocabulary(documents):\n",
    "    # Create the vocabulary\n",
    "    # flatten 'documents'\n",
    "    vocabulary = set([item for sublist in documents\n",
    "        for item in sublist])\n",
    "    # remove stopwords and sort\n",
    "    vocabulary = [word for word in vocabulary if\n",
    "        word not in stopwords.words('english')]\n",
    "    vocabulary.sort()\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# Compute IDF, storing values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"Computes IDF as log(N/nt)\n",
    "    where nt is the number of documents where the\n",
    "    term t appears.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for term in vocabulary:\n",
    "        idf[term] = [term in x for x in documents]\n",
    "        idf[term] = math.log(num_documents /\n",
    "            Counter(idf[term])[True], math.e)\n",
    "    return idf\n",
    "\n",
    "\n",
    "# Generate the vector for a document (with normalization)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"Returns TF-IDF as TF*IDF\n",
    "    Computes TF as the term frequency, which means\n",
    "    ftd/max_count\n",
    "    where:\n",
    "        ftd = nb. of times term t occurs in doc d\n",
    "        max_count = nb. of times most common term occurs in d\n",
    "    This is known as 'augmented frequency' and prevents\n",
    "    a bias towards longer documents.\n",
    "    \"\"\"\n",
    "    tfidf = [0] * len(vocabulary)\n",
    "    tf = [0] * len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    # get the count for most common word\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        tf[i] = counts[term] / max_count \n",
    "        tfidf[i] = tf[i] * idf[term]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]\n",
    "        y = v2[i]\n",
    "        sumxx += x * x\n",
    "        sumyy += y * y\n",
    "        sumxy += x * y\n",
    "    if sumxy == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sumxy / math.sqrt(sumxx * sumyy)\n",
    "\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    # get array of vectorized query words\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    # get the vector for the query\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "\n",
    "    \n",
    "# Compute the search result (get topk documents)\n",
    "def search_vec(query, topk, vocabulary, idf, document_vectors,\n",
    "    original_documents):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    # compute cosine similarity scores\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d]\n",
    "        for d in range(len(documents))]\n",
    "    # sort and return search result\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(topk):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "        \n",
    "# Put everything together\n",
    "def search_scores_vec(query, topk, vocabulary, documents, original_documents):\n",
    "    idf = idf_values(vocabulary, documents)\n",
    "    document_vectors = [vectorize(d, vocabulary, idf) for d in documents]\n",
    "    ans, indices, query_vector = search_vec(query, topk, vocabulary,\n",
    "        idf, document_vectors, original_documents)\n",
    "    for d in ans:\n",
    "        print(d)\n",
    "    return ans, indices, query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with baking and compare with scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Smith Pies: Best Pies in London\n",
      "Numerical Recipes: The Art of Scientific Computing\n",
      "Pastry: A Book of Best French Pastry Recipes\n"
     ]
    }
   ],
   "source": [
    "documents, original_documents = read_documents(\"bread.txt\")\n",
    "ans, indices, query_vector = search_scores_vec(\"baking\", 5,\n",
    "    create_vocabulary(documents), documents, original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Pastry: A Book of Best French Pastry Recipes\n",
      "Numerical Recipes: The Art of Scientific Computing\n",
      "Smith Pies: Best Pies in London\n"
     ]
    }
   ],
   "source": [
    "# Reference code using scikit-learn\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "new_features = tf.transform(['baking'])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "topk = 5\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with computer science and compare with scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n",
      "Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\n",
      "@CodeWeekEU is turning 5, yay! We look very much forward to computational thinking unplugged activities during @CodeWeek_CH https://t.co/yDPrlKg4hw\n",
      "Le mystère Soulages éblouit la science @EPFL  https://t.co/u3uNICyAdi\n",
      "@cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!\n",
      "Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl\n"
     ]
    }
   ],
   "source": [
    "documents, original_documents = read_documents(\"epfldocs.txt\")\n",
    "ans, indices, query_vector = search_scores_vec(\"computer science\", 10,\n",
    "    create_vocabulary(documents), documents, original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\n",
      "Le mystère Soulages éblouit la science @EPFL  https://t.co/u3uNICyAdi\n",
      "@cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!\n",
      "Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl\n",
      "Swiss Data Science on Twitter: \"Sign up for @EPFL_en #DataJamDays: learn more a… https://t.co/kNVILHWPGb, see more https://t.co/2wg3BbHBNq\n",
      "The registration for Exposure Science Film Hackathon 2017 is open! #scicomm #lausanne #epfl #unil https://t.co/mY5jlwsXUD\n",
      "Know someone who has promoted sound #science? Less than 2 weeks to nominate them for the #MaddoxPrize. https://t.co/POnZtf3vFT\n"
     ]
    }
   ],
   "source": [
    "# Reference code using scikit-learn\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "new_features = tf.transform(['computer science'])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "topk = 10\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2\n",
    "\n",
    "Implement probabilistic retrieval model based on the query likelihood language model, using a mixture model between the documents and the collection, both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. You can use the code framework provided below.\n",
    "\n",
    "Now, for the query $Q$ = `\"baking\"`, find the top ranked documents according to the probabilistic rank.\n",
    "\n",
    "Compare the results with TF-IDF ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    \"\"\"Given a string, removes the punctuation and returns\n",
    "    array of stemmed words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = \"\".join([ch for ch in text if ch\n",
    "        not in string.punctuation])\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Return stemmed document\n",
    "    return \" \".join([stemmer.stem(word.lower())\n",
    "        for word in tokens])\n",
    "\n",
    "\n",
    "def read_documents(filename):\n",
    "    # Read documents from file (each line is a document)\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    original_documents = [x.strip() for x in content] \n",
    "    documents = [tokenize(d).split() for d in original_documents]\n",
    "    return documents, original_documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ref.: https://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf\n",
    "# The query likelihood model returns results ranked by\n",
    "# P(q|d), i.e. the probability of the query q under\n",
    "# the language model derived from d. \n",
    "\n",
    "# probabilistic relevance\n",
    "def query_prob(q, document, documents, lambda_):\n",
    "    \"\"\"Probability of producing the query given:\n",
    "        Md = language model of document d\n",
    "        Ld = Number of tokens in document d\n",
    "    Score is >0 iff all query terms appear in the document\n",
    "    \"\"\"\n",
    "    # term frequency in document\n",
    "    tf = Counter(document)\n",
    "    # length of document d\n",
    "    L_doc = len(document)\n",
    "    # term frequency in collection\n",
    "    collection = [w for doc in documents for w in doc]\n",
    "    cf = Counter(collection)\n",
    "    # length of collection\n",
    "    L_col = len(collection)\n",
    "\n",
    "    # compute prob\n",
    "    prob = 1\n",
    "    for term in q:\n",
    "        prob = prob * (lambda_ * (tf[term] / L_doc) +\n",
    "            (1 - lambda_) * cf[term] / L_col)\n",
    "    return prob\n",
    "\n",
    "# computing the search result\n",
    "def search_prob(query, k, documents, original_documents):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    scores = [[query_prob(q, documents[d], documents, 0.5), d]\n",
    "        for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(k):\n",
    "        print(original_documents[scores[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents, original_documents = read_documents(\"epfldocs.txt\")\n",
    "search_prob(\"computer science\", 10, documents, original_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Following the notation used in class, let us denote the set of terms by $T=\\{k_i|i=1,...,m\\}$, the set of documents by $D=\\{d_j |j=1,...,n\\}$, and let $d_i=(w_{1j},w_{2j},...,w_{mj})$. We are also given a query  $q=(w_{1q},w_{2q},...,w_{mq})$. In the lecture we studied that, \n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} \\frac{w_{ij}}{|d_j|}\\frac{w_{iq}}{|q|}$ .  (1)\n",
    "\n",
    "Another way of looking at the information retrieval problem is using a probabilistic approach. The probabilistic view of information retrieval consists of determining the conditional probability $P(q|d_j)$ that for a given document $d_j$ the query by the user is $q$. So, practically in probabilistic retrieval when a query $q$ is given, for each document it is evaluated how probable it is that the query is indeed relevant for the document, which results in a ranking of the documents.\n",
    "\n",
    "In order to relate vector space retrieval to a probabilistic view of information retrieval, we interpret the weights in Equation (1) as follows:\n",
    "\n",
    "-  $w_{ij}/|d_j|$ can be interpreted as the conditional probability $P(k_i|d_j)$ that for a given document $d_j$ the term $k_i$ is important (to characterize the document $d_j$).\n",
    "\n",
    "-  $w_{iq}/|q|$ can be interpreted as the conditional probability $P(q|k_i)$ that for a given term $k_i$ the query posed by the user is $q$. Intuitively, $P(q|k_i)$ gives the amount of importance given to a particular term while querying.\n",
    "\n",
    "With this interpretation you can rewrite Equation (1) as follows:\n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} P(k_i|d_j)P(q|k_i)$ (2)\n",
    "\n",
    "### Question a\n",
    "Show that indeed with the probabilistic interpretation of weights of vector space retrieval, as given in Equation (2), the similarity computation in vector space retrieval results exactly in the probabilistic interpretation of information retrieval, i.e., $sim(q,d_j)= P(q|d_j)$.\n",
    "Given that $d_j$ and $q$ are conditionally independent, i.e., $P(d_j \\cap q|ki) = P(d_j|k_i)P(q|k_i)$. You can assume existence of joint probability density functions wherever required. (Hint: You might need to use Bayes theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b\n",
    "Using the expression derived for $P(q|d_j)$ in (a), obtain a ranking (documents sorted in descending order of their scores) for the documents $P(k_i|d_1) = (0, 1/3, 2/3)$, $P(k_i|d_2) =(1/3, 2/3, 0)$, $P(k_i|d_3) = (1/2, 0, 1/2)$, and $P (k_i|d_4) = (3/4, 1/4, 0)$ and the query $P(q|k_i) = (1/5, 0, 2/3)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
